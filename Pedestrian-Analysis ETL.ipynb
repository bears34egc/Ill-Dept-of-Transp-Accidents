{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecoker/.local/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from shapely.geometry import Polygon, Point\n",
    "from geopy.distance import great_circle\n",
    "from geopy.point import Point\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "#from geopandas import GeoDataFrame\n",
    "#from descartes import PolygonPatch\n",
    "import geopy\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import notebook\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "E = notebook.nbextensions.EnableNBExtensionApp()\n",
    "E.enable_nbextension('usability/codefolding/main')\n",
    "E.enable_nbextension('usability/hide_input/main')\n",
    "E.enable_nbextension('usability/hide_input_all/main')\n",
    "E.enable_nbextension('usability/highlighter/highlighter')\n",
    "E.enable_nbextension('usability/toggle_all_line_numbers/main')\n",
    "E.enable_nbextension('usability/rubberband/main')\n",
    "E.enable_nbextension('usability/execute_time/ExecuteTime')\n",
    "from lightning import Lightning\n",
    "#lgn = Lightning(ipython=True, local=True)\n",
    "import codecs, json ,base64\n",
    "import simplejson\n",
    "import IPython.core.display as di\n",
    "from ggplot import *\n",
    "from shapely.geometry import Polygon, Point\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "import fiona\n",
    "from matplotlib.collections import PatchCollection\n",
    "from descartes import PolygonPatch\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  This reads in the Crash from 2012 thru 2014 data, filters it to Chicago and only looks at those which are Pedestrian or Pedacyclist\n",
    "\n",
    "frame=pd.read_csv('CrashExtract2014.txt',error_bad_lines=False)\n",
    "frame2=pd.read_csv('CrashExtract2013.txt',error_bad_lines=False)\n",
    "frame3=pd.read_csv('CrashExtract2012.txt',error_bad_lines=False)\n",
    "frame=frame.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame2=frame2.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame3=frame3.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frame2.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frame3.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frames=pd.concat([frame,frame2,frame3],axis=0)\n",
    "frames=frames[frames.city==3]\n",
    "frames=frames[(frames.accident_type==1) | (frames.accident_type==2)]\n",
    "frames['longitude']=frames['longitude']*-1\n",
    "frames['datetime_accident'] = pd.to_datetime(frames['dayte']+ ' ' + frames['times'])\n",
    "frames['daytetime'] = frames['datetime_accident'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour))\n",
    "colskeep = ['id','daytetime','datetime_accident','accident_type','road_surface','lighting','weather','intersection','hit_run','trafficway','roadway','latitude','longitude',\\\n",
    "           'day_of_wk','cause1','cause2','crash_severity']\n",
    "frames = frames.loc[:,colskeep]\n",
    "#frames.info()\n",
    "pickle.dump(frames, open( \"accident_info_2012-13-14.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## City of Chicago Liquor Licenses data use geolocator to add Coordinates\n",
    "\n",
    "liqu = pd.read_csv('Business_Licenses_-_Current_Liquor_and_Public_Places_of_Amusement_Licenses (1).csv',infer_datetime_format=True,parse_dates=['DATE ISSUED'])\n",
    "liqu = liqu.loc[:,['DOING BUSINESS AS NAME','ADDRESS','DATE ISSUED','LATITUDE','LONGITUDE']]\n",
    "liqu.columns = ['biz_name','address','daytetime','latitude','longitude']\n",
    "liqfix = liqu[liqu.latitude.isnull()]\n",
    "liqu = liqu.drop_duplicates().dropna()\n",
    "\n",
    "geolocator = Nominatim()\n",
    "try:\n",
    "    liqfix['coords'] = (liqfix['address']).apply(geolocator.geocode,timeout=1000)\n",
    "except GeocoderTimedOut as e:\n",
    "    print(\"Error: geocode failed on input %s with message %s\"%(liqfix['coords'], e.msg))\n",
    "liqcols = ['biz_name','address','daytetime','latitude','longitude']\n",
    "liqu = liqu.loc[:,liqcols]\n",
    "liqfix = liqfix.loc[:,liqcols]\n",
    "liq = pd.concat([liqu,liqfix],axis=0)\n",
    "liq = liq.dropna(subset=['latitude','longitude'])\n",
    "liq = liq[liq.latitude.notnull()]\n",
    "liq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Traffic Segments by Coordinate (polygon)\n",
    "segments = pd.read_csv('Segments.csv')\n",
    "segments.head()\n",
    "segments.columns=['segment_id','STREET','direction','FROM_STREET','TO_STREET','length','STREET_HEADING','COMMENTS','START_LONGITUDE','START_LATITUDE','END_LONGITUDE','END_LATITUDE','CURRENT_SPEED','datetime']\n",
    "segments['lon']=((segments['START_LONGITUDE']+segments['END_LONGITUDE']) / 2)\n",
    "segments['lat']=((segments['START_LATITUDE']+segments['END_LATITUDE'] / 2))\n",
    "keeping = ['segment_id','direction','lat','lon','length','START_LATITUDE','END_LATITUDE','START_LONGITUDE','END_LONGITUDE']\n",
    "segments = segments.loc[:,keeping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "## Historical Congestion from City of Chicago Data\n",
    "ctcs = pd.read_csv('Historical_Congestion_by_Seg.csv',infer_datetime_format=True,parse_dates=['daytetime'])\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%m/%d/%Y %H:%M:%S %p')\n",
    "ctcs2 = pd.read_csv('Chicago_Traffic_Tracker_-_Historical_Congestion_Estimates_by_Segment (1).csv',infer_datetime_format=True,parse_dates=['TIME'],date_parser=dateparse,engine='c')\n",
    "ctcs2.columns = ['daytetime', 'segment_id', 'bus_ct', 'message_ct', 'speed', 'id']\n",
    "tokeep=['daytetime','segment_id','speed']\n",
    "ctcs2=ctcs2.loc[:,tokeep]\n",
    "#ctcs.to_csv('Historical_Congestion_by_Seg.csv',index=False)\n",
    "cong = pd.DataFrame()\n",
    "cong = pd.concat([ctcs,ctcs2],axis=0)\n",
    "traffic = pd.merge(cong,segments,how='inner',on=['segment_id'])\n",
    "pickle.dump(traffic, open( \"traffic.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Use Cython to create datetime part calculations. Cython is used frequently along with pickle to make some dataframe operations quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "traffic = pickle.load( open( \"traffic.p\", \"rb\" ) )\n",
    "traffic['daytetime'] = pd.to_datetime(traffic['daytetime'])\n",
    "traffic['dayte'] = traffic['daytetime'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day))\n",
    "traffic['hour'] = traffic['daytetime'].apply(lambda x: x.hour)\n",
    "traffic['dow'] = traffic['daytetime'].apply(lambda x: x.weekday())\n",
    "traffic['avg_day_spd'] = traffic.groupby(['segment_id','dow','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['avg_dly_spd'] = traffic.groupby(['segment_id','dayte','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['avg_h_spd'] = traffic.groupby(['segment_id','hour','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['datetime_recorded'] = traffic['daytetime']\n",
    "traffic['daytetime'] = traffic['datetime_recorded'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour))\n",
    "pickle.dump(traffic, open( \"traffic2.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will start to merge traffic with accident data. Then, bring in liquor data and use Cython/pickling to calculate distances between accidents and liquor establishments nearby.\n",
    "traffic = pickle.load( open( \"traffic2.p\", \"rb\" ) )\n",
    "merge1 = pd.merge(traffic,frames,how='right',on='daytetime')\n",
    "merge1 = merge1.drop_duplicates(subset=['id','avg_day_spd','avg_dly_spd','avg_h_spd'])\n",
    "mergekeep = ['latitude','longitude','id']\n",
    "merge1.loc[:,mergekeep].head()\n",
    "geocolsframe = pd.DataFrame()\n",
    "geocolsframe2 = pd.DataFrame()\n",
    "geoframe = pd.DataFrame()\n",
    "mergekeep = ['latitude','longitude','id']\n",
    "geocolsframe = merge1.loc[:,mergekeep]\n",
    "geocolsframe = geocolsframe.drop_duplicates()\n",
    "liqcols = ['biz_name','latitude','longitude']\n",
    "geocolsframe2 = liq.loc[:,liqcols]\n",
    "geoframe = pd.concat([geocolsframe,geocolsframe2],axis=0)\n",
    "#geoframe['coords']=zip(geoframe.latitude,geoframe.longitude)\n",
    "#del geoframe['latitude'],geoframe['longitude']\n",
    "del geocolsframe,geocolsframe2\n",
    "\n",
    "geoframe['id'] = geoframe['id'].apply(lambda x: 'id' + ' ' + str(x))\n",
    "geoframe['biz_name'] = geoframe['biz_name'].apply(lambda x: 'biz_name' + ' ' + str(x))\n",
    "geoframe = pd.melt(geoframe,id_vars=['latitude','longitude'])\n",
    "print geoframe.head()\n",
    "\n",
    "geoframe = geoframe[~geoframe['value'].str.contains('nan')]\n",
    "geoframe = geoframe.dropna()\n",
    "del geoframe['variable']\n",
    "\n",
    "geoframe = geoframe.drop_duplicates()\n",
    "pickle.dump(geoframe, open( \"coords.p\", \"wb\" ))\n",
    "\n",
    "geoframe_single = geoframe.value.drop_duplicates()\n",
    "pickle.dump(geoframe_single, open( \"geoframe.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def expand_grid(x, y):\n",
    "    xG, yG = np.meshgrid(x, y) # create the actual grid\n",
    "    xG = xG.flatten() # make the grid 1d\n",
    "    yG = yG.flatten() # same\n",
    "    return pd.DataFrame({'x':xG, 'y':yG})\n",
    "\n",
    "geoframe_single = pickle.load( open( \"geoframe.p\", \"rb\" ) )\n",
    "liquor = geoframe_single[geoframe_single.str.startswith('biz_name')]\n",
    "accident = geoframe_single[geoframe_single.str.startswith('id')]\n",
    "liquor = liquor.dropna()\n",
    "accident = accident.dropna()\n",
    "liq = liquor.as_matrix()\n",
    "acc = accident.as_matrix()\n",
    "\n",
    "distances = expand_grid(liq, acc)\n",
    "pickle.dump(distances, open(\"save.p\", \"wb\"))\n",
    "print len(distances),distances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "import geopy\n",
    "from geopy.distance import great_circle\n",
    "from itertools import product\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "coords = pickle.load( open( \"coords.p\", \"rb\" ) )\n",
    "distances = pickle.load(open(\"save.p\", \"rb\"))\n",
    "distances.columns = ['biz_name','id']\n",
    "liquor = coords[coords.value.str.startswith('biz_name')]\n",
    "accident = coords[coords.value.str.startswith('id')]\n",
    "distance_frame_pre = pd.DataFrame()\n",
    "distance_frame_pre2 = pd.DataFrame()\n",
    "distance_frame_pre = pd.merge(liquor,distances,how='right',left_on='value',right_on='biz_name')\n",
    "distance_frame_pre2 = pd.merge(accident,distances,how='right',left_on='value',right_on='id')\n",
    "distance_frame_pre.columns = ['biz_lat','biz_lon','value','biz_name','id']\n",
    "del distance_frame_pre['value']\n",
    "distance_frame_pre2.columns = ['id_lat','id_lon','value','biz_name','id']\n",
    "del distance_frame_pre2['value']\n",
    "distances_frame = pd.merge(distance_frame_pre,distance_frame_pre2,how='inner',on=['biz_name','id']).dropna()\n",
    "pickle.dump(distances_frame, open( \"distances.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "import geopy\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "distances_frame = pickle.load(open(\"distances.p\", \"rb\"))\n",
    "distances_frame['distances'] = distances_frame.apply(lambda x: great_circle((x['biz_lat'],x['biz_lon']), (x['id_lat'], x['id_lon'])).miles, axis=1)\n",
    "thecols = ['biz_name','id','distances']\n",
    "distances_frame = distances_frame.loc[:,thecols].drop_duplicates()\n",
    "distance_data = pd.DataFrame()\n",
    "distance_data = distances_frame[distances_frame['distances'] < 1.1 ]\n",
    "distance_data['ct_1m'] = distance_data.groupby('id')['biz_name'].transform(lambda x: x.nunique())\n",
    "pickle.dump(distance_data,open(\"distance_data.p\",\"wb\"))\n",
    "pickle.dump(distances_frame, open( \"distances_full.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
