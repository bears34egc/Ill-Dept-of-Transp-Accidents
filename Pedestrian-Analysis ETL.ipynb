{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecoker/.local/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from shapely.geometry import Polygon, Point\n",
    "from geopy.distance import great_circle\n",
    "from geopy.point import Point\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "#from geopandas import GeoDataFrame\n",
    "#from descartes import PolygonPatch\n",
    "import geopy\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import notebook\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "E = notebook.nbextensions.EnableNBExtensionApp()\n",
    "E.enable_nbextension('usability/codefolding/main')\n",
    "E.enable_nbextension('usability/hide_input/main')\n",
    "E.enable_nbextension('usability/hide_input_all/main')\n",
    "E.enable_nbextension('usability/highlighter/highlighter')\n",
    "E.enable_nbextension('usability/toggle_all_line_numbers/main')\n",
    "E.enable_nbextension('usability/rubberband/main')\n",
    "E.enable_nbextension('usability/execute_time/ExecuteTime')\n",
    "from lightning import Lightning\n",
    "#lgn = Lightning(ipython=True, local=True)\n",
    "import codecs, json ,base64\n",
    "import simplejson\n",
    "import IPython.core.display as di\n",
    "from ggplot import *\n",
    "from shapely.geometry import Polygon, Point\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "import fiona\n",
    "from matplotlib.collections import PatchCollection\n",
    "from descartes import PolygonPatch\n",
    "import json\n",
    "import datetime\n",
    "import IPython.core.display as di\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)\n",
    "from IPython.core.display import HTML\n",
    "def ldf_display(df, lines=15):\n",
    "    txt = (\"<iframe \" +\n",
    "           \"srcdoc='\" + df.head(lines).to_html() + \"' \" +\n",
    "           \"width=1900 height=600>\" +\n",
    "           \"</iframe>\")\n",
    "\n",
    "    return HTML(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 24153: expected 77 fields, saw 78\n",
      "\n",
      "Skipping line 48340: expected 77 fields, saw 79\n",
      "\n",
      "Skipping line 270591: expected 77 fields, saw 78\n",
      "\n",
      "Skipping line 294931: expected 77 fields, saw 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##  This reads in the Crash from 2012 thru 2014 data, filters it to Chicago and only looks at those which are Pedestrian or Pedalcyclist\n",
    "\n",
    "frame=pd.read_csv('CrashExtract2014.txt',error_bad_lines=False)\n",
    "frame2=pd.read_csv('CrashExtract2013.txt',error_bad_lines=False)\n",
    "frame3=pd.read_csv('CrashExtract2012.txt',error_bad_lines=False)\n",
    "frame=frame.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame2=frame2.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame3=frame3.iloc[:,(1,10,12,13,14,26,28,29,33,35,36,37,40,41,46,47,48,49,54,55,60,64,67)]\n",
    "frame.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frame2.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frame3.columns=['id','city','accident_type','total_killed','total_injured','road_surface','lighting','weather','times','intersection','hit_run','dayte','trafficway','roadway','latitude','longitude','county','day_of_wk','cause1','cause2','crash_severity','trafficway_d','crash_severity_cd']\n",
    "frames=pd.concat([frame,frame2,frame3],axis=0)\n",
    "frames=frames[frames.city==3]\n",
    "frames=frames[(frames.accident_type==1) | (frames.accident_type==2)]\n",
    "frames['longitude']=frames['longitude']*-1\n",
    "frames['datetime_accident'] = pd.to_datetime(frames['dayte']+ ' ' + frames['times'])\n",
    "frames['daytetime'] = frames['datetime_accident'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour))\n",
    "colskeep = ['id','daytetime','datetime_accident','accident_type','road_surface','lighting','weather','intersection','hit_run','trafficway','roadway','latitude','longitude',\\\n",
    "           'day_of_wk','cause1','cause2','crash_severity']\n",
    "frames = frames.loc[:,colskeep]\n",
    "#frames.info()\n",
    "pickle.dump(frames, open( \"accident_info_2012-13-14.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biz_name</th>\n",
       "      <th>address</th>\n",
       "      <th>daytetime</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENCHMARK BAR &amp; GRILL</td>\n",
       "      <td>1508-1510 N WELLS ST 1ST AND 2ND FLOORS</td>\n",
       "      <td>2014-04-14</td>\n",
       "      <td>41.909474</td>\n",
       "      <td>-87.634804</td>\n",
       "      <td>Tavern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROSCOE'S TAVERN LTD</td>\n",
       "      <td>3354-3356 N HALSTED ST</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>41.943443</td>\n",
       "      <td>-87.649517</td>\n",
       "      <td>Outdoor Patio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FISHBAR</td>\n",
       "      <td>2956 N SHEFFIELD AVE 1</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>41.936160</td>\n",
       "      <td>-87.654127</td>\n",
       "      <td>Consumption on Premises - Incidental Activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLACO'S TACOS</td>\n",
       "      <td>46 E CHICAGO AVE 1</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>41.896811</td>\n",
       "      <td>-87.626449</td>\n",
       "      <td>Consumption on Premises - Incidental Activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAO THAI RESTRAUNT</td>\n",
       "      <td>230 E OHIO ST  1ST</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>41.892690</td>\n",
       "      <td>-87.621438</td>\n",
       "      <td>Consumption on Premises - Incidental Activity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                biz_name                                  address  daytetime  \\\n",
       "0  BENCHMARK BAR & GRILL  1508-1510 N WELLS ST 1ST AND 2ND FLOORS 2014-04-14   \n",
       "2    ROSCOE'S TAVERN LTD                   3354-3356 N HALSTED ST 2015-02-13   \n",
       "3                FISHBAR                   2956 N SHEFFIELD AVE 1 2015-02-13   \n",
       "4          FLACO'S TACOS                       46 E CHICAGO AVE 1 2015-02-13   \n",
       "5     DAO THAI RESTRAUNT                       230 E OHIO ST  1ST 2015-02-13   \n",
       "\n",
       "    latitude  longitude                                           type  \n",
       "0  41.909474 -87.634804                                         Tavern  \n",
       "2  41.943443 -87.649517                                  Outdoor Patio  \n",
       "3  41.936160 -87.654127  Consumption on Premises - Incidental Activity  \n",
       "4  41.896811 -87.626449  Consumption on Premises - Incidental Activity  \n",
       "5  41.892690 -87.621438  Consumption on Premises - Incidental Activity  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## City of Chicago Liquor Licenses data use geolocator to add Coordinates\n",
    "\n",
    "liquor = pd.read_csv('Business_Licenses_-_Current_Liquor_and_Public_Places_of_Amusement_Licenses (1).csv',infer_datetime_format=True,parse_dates=['DATE ISSUED'])\n",
    "liqu = liquor.loc[:,['DOING BUSINESS AS NAME','ADDRESS','DATE ISSUED','LATITUDE','LONGITUDE','LICENSE DESCRIPTION']]\n",
    "liqu.rename(columns=lambda x: x.replace(' ','_'), inplace=True)\n",
    "liqu.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "liqu.columns = ['biz_name','address','daytetime','latitude','longitude','type']\n",
    "liqu = liqu[~liqu['type'].isin([\"Caterer's Liquor License\",\"Caterer's Registration (Liquor)\",\"Class A - Indoor Special Event\",\"Package Goods\"])]\n",
    "liq = liqu.drop_duplicates(subset=['biz_name','address']).dropna()\n",
    "#liqfix = liqu[liqu.longitude.isnull()]\n",
    "\n",
    "# geolocator = Nominatim()\n",
    "# try:\n",
    "#     liqfix['coords'] = (liqfix['address']).apply(geolocator.geocode,timeout=1000)\n",
    "# except GeocoderTimedOut as e:\n",
    "#     print(\"Error: geocode failed on input %s with message %s\"%(liqfix['coords'], e.msg))\n",
    "# # liqcols = ['biz_name','address','daytetime','latitude','longitude']\n",
    "liq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Traffic Segments by Coordinate (polygon)\n",
    "segments = pd.read_csv('Segments.csv')\n",
    "segments.head()\n",
    "segments.columns=['segment_id','STREET','direction','FROM_STREET','TO_STREET','length','STREET_HEADING','COMMENTS','START_LONGITUDE','START_LATITUDE','END_LONGITUDE','END_LATITUDE','CURRENT_SPEED','datetime']\n",
    "segments['lon']=((segments['START_LONGITUDE']+segments['END_LONGITUDE']) / 2)\n",
    "segments['lat']=((segments['START_LATITUDE']+segments['END_LATITUDE'] / 2))\n",
    "keeping = ['segment_id','direction','lat','lon','length','START_LATITUDE','END_LATITUDE','START_LONGITUDE','END_LONGITUDE']\n",
    "segments = segments.loc[:,keeping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "## Historical Congestion from City of Chicago Data\n",
    "ctcs = pd.read_csv('Historical_Congestion_by_Seg.csv',infer_datetime_format=True,parse_dates=['daytetime'])\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%m/%d/%Y %H:%M:%S %p')\n",
    "ctcs2 = pd.read_csv('Chicago_Traffic_Tracker_-_Historical_Congestion_Estimates_by_Segment (1).csv',infer_datetime_format=True,parse_dates=['TIME'],date_parser=dateparse,engine='c')\n",
    "ctcs2.columns = ['daytetime', 'segment_id', 'bus_ct', 'message_ct', 'speed', 'id']\n",
    "tokeep=['daytetime','segment_id','speed']\n",
    "ctcs2=ctcs2.loc[:,tokeep]\n",
    "#ctcs.to_csv('Historical_Congestion_by_Seg.csv',index=False)\n",
    "cong = pd.DataFrame()\n",
    "cong = pd.concat([ctcs,ctcs2],axis=0)\n",
    "traffic = pd.merge(cong,segments,how='inner',on=['segment_id'])\n",
    "pickle.dump(traffic, open( \"traffic.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Use Cython to create datetime part calculations. Cython is used frequently along with pickle to make some dataframe operations quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "traffic = pickle.load( open( \"traffic.p\", \"rb\" ) )\n",
    "traffic['daytetime'] = pd.to_datetime(traffic['daytetime'])\n",
    "traffic['dayte'] = traffic['daytetime'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day))\n",
    "traffic['hour'] = traffic['daytetime'].apply(lambda x: x.hour)\n",
    "traffic['dow'] = traffic['daytetime'].apply(lambda x: x.weekday())\n",
    "traffic['avg_day_spd'] = traffic.groupby(['segment_id','dow','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['avg_dly_spd'] = traffic.groupby(['segment_id','dayte','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['avg_h_spd'] = traffic.groupby(['segment_id','hour','direction'])['speed'].transform(lambda x: x.mean())\n",
    "traffic['datetime_recorded'] = traffic['daytetime']\n",
    "traffic['daytetime'] = traffic['datetime_recorded'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour))\n",
    "pickle.dump(traffic, open( \"traffic2.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude  longitude  variable         value\n",
      "0  41.880570 -87.641119  biz_name  biz_name nan\n",
      "1  42.011979 -87.704445  biz_name  biz_name nan\n",
      "2  41.851972 -87.690538  biz_name  biz_name nan\n",
      "3  41.910050 -87.713128  biz_name  biz_name nan\n",
      "4  41.983227 -87.668687  biz_name  biz_name nan\n"
     ]
    }
   ],
   "source": [
    "## This will start to merge traffic with accident data. Then, bring in liquor data and use Cython/pickling to calculate distances between accidents and liquor establishments nearby.\n",
    "traffic = pickle.load( open( \"traffic2.p\", \"rb\" ) )\n",
    "merge1 = pd.merge(traffic,frames,how='right',on='daytetime')\n",
    "merge1 = merge1.drop_duplicates(subset=['id','avg_day_spd','avg_dly_spd','avg_h_spd'])\n",
    "mergekeep = ['latitude','longitude','id']\n",
    "merge1.loc[:,mergekeep].head()\n",
    "geocolsframe = pd.DataFrame()\n",
    "geocolsframe2 = pd.DataFrame()\n",
    "geoframe = pd.DataFrame()\n",
    "mergekeep = ['latitude','longitude','id']\n",
    "geocolsframe = merge1.loc[:,mergekeep]\n",
    "geocolsframe = geocolsframe.drop_duplicates()\n",
    "liqcols = ['biz_name','latitude','longitude']\n",
    "geocolsframe2 = liq.loc[:,liqcols]\n",
    "geoframe = pd.concat([geocolsframe,geocolsframe2],axis=0)\n",
    "#geoframe['coords']=zip(geoframe.latitude,geoframe.longitude)\n",
    "#del geoframe['latitude'],geoframe['longitude']\n",
    "del geocolsframe,geocolsframe2\n",
    "\n",
    "geoframe['id'] = geoframe['id'].apply(lambda x: 'id' + ' ' + str(x))\n",
    "geoframe['biz_name'] = geoframe['biz_name'].apply(lambda x: 'biz_name' + ' ' + str(x))\n",
    "geoframe = pd.melt(geoframe,id_vars=['latitude','longitude'])\n",
    "print geoframe.head()\n",
    "\n",
    "geoframe = geoframe[~geoframe['value'].str.contains('nan')]\n",
    "geoframe = geoframe.dropna()\n",
    "del geoframe['variable']\n",
    "\n",
    "geoframe = geoframe.drop_duplicates()\n",
    "pickle.dump(geoframe, open( \"coords.p\", \"wb\" ))\n",
    "\n",
    "geoframe_single = geoframe.value.drop_duplicates()\n",
    "pickle.dump(geoframe_single, open( \"geoframe.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def expand_grid(x, y):\n",
    "    xG, yG = np.meshgrid(x, y) # create the actual grid\n",
    "    xG = xG.flatten() # make the grid 1d\n",
    "    yG = yG.flatten() # same\n",
    "    return pd.DataFrame({'x':xG, 'y':yG})\n",
    "\n",
    "geoframe_single = pickle.load( open( \"geoframe.p\", \"rb\" ) )\n",
    "liquor = geoframe_single[geoframe_single.str.startswith('biz_name')]\n",
    "accident = geoframe_single[geoframe_single.str.startswith('id')]\n",
    "liquor = liquor.dropna()\n",
    "accident = accident.dropna()\n",
    "liq = liquor.as_matrix()\n",
    "acc = accident.as_matrix()\n",
    "\n",
    "distances = expand_grid(liq, acc)\n",
    "pickle.dump(distances, open(\"save.p\", \"wb\"))\n",
    "print len(distances),distances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "import geopy\n",
    "from geopy.distance import great_circle\n",
    "from itertools import product\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "coords = pickle.load( open( \"coords.p\", \"rb\" ) )\n",
    "distances = pickle.load(open(\"save.p\", \"rb\"))\n",
    "distances.columns = ['biz_name','id']\n",
    "liquor = coords[coords.value.str.startswith('biz_name')]\n",
    "accident = coords[coords.value.str.startswith('id')]\n",
    "distance_frame_pre = pd.DataFrame()\n",
    "distance_frame_pre2 = pd.DataFrame()\n",
    "distance_frame_pre = pd.merge(liquor,distances,how='right',left_on='value',right_on='biz_name')\n",
    "distance_frame_pre2 = pd.merge(accident,distances,how='right',left_on='value',right_on='id')\n",
    "distance_frame_pre.columns = ['biz_lat','biz_lon','value','biz_name','id']\n",
    "del distance_frame_pre['value']\n",
    "distance_frame_pre2.columns = ['id_lat','id_lon','value','biz_name','id']\n",
    "del distance_frame_pre2['value']\n",
    "distances_frame = pd.merge(distance_frame_pre,distance_frame_pre2,how='inner',on=['biz_name','id']).dropna()\n",
    "pickle.dump(distances_frame, open( \"distances.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "import geopy\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "distances_frame = pickle.load(open(\"distances.p\", \"rb\"))\n",
    "distances_frame['distances'] = distances_frame.apply(lambda x: great_circle((x['biz_lat'],x['biz_lon']), (x['id_lat'], x['id_lon'])).miles, axis=1)\n",
    "thecols = ['biz_name','id','distances']\n",
    "distances_frame = distances_frame.loc[:,thecols].drop_duplicates()\n",
    "distance_data = pd.DataFrame()\n",
    "distance_data = distances_frame[distances_frame['distances'] < 1.1 ]\n",
    "distance_data['ct_1m'] = distance_data.groupby('id')['biz_name'].transform(lambda x: x.nunique())\n",
    "pickle.dump(distance_data,open(\"distance_data.p\",\"wb\"))\n",
    "pickle.dump(distances_frame, open( \"distances_full.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(frames, open( \"accident_info_2012-13-14.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
